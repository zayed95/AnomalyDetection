"use strict";(self.webpackChunk_mlflow_mlflow=self.webpackChunk_mlflow_mlflow||[]).push([[5167],{14429:function(e,t,n){n.d(t,{UY:function(){return m},VA:function(){return c},oy:function(){return d},vF:function(){return s}});var o=n(46709),a=n(91105),i=n(79444);const s="startTimeLabel",l="startTime",r="endTime",d="LAST_7_DAYS",c=()=>{var e;const t=(0,i.U)(),[n,c]=(0,a.ok)(),u=n.get(s)||d;let g=n.get(l)||void 0,p=null!==(e=n.get(r))&&void 0!==e?e:void 0;if("CUSTOM"!==u){const e=m(t.dateNow,{startTimeLabel:u});g=e.startTime,p=e.endTime}else{var h;g=n.get(l)||void 0,p=null!==(h=n.get(r))&&void 0!==h?h:void 0}const f=(0,o.useMemo)((()=>({startTimeLabel:u,startTime:g,endTime:p})),[u,g,p]),T=(0,o.useCallback)(((e,t=!1)=>{c((t=>(void 0===(null===e||void 0===e?void 0:e.startTime)?t.delete(l):"CUSTOM"===e.startTimeLabel&&t.set(l,e.startTime),void 0===(null===e||void 0===e?void 0:e.endTime)?t.delete(r):"CUSTOM"===e.startTimeLabel&&t.set(r,e.endTime),void 0===(null===e||void 0===e?void 0:e.startTimeLabel)?t.delete(s):t.set(s,e.startTimeLabel),t)),{replace:t})}),[c]);return[f,T]};function m(e,t){return t.startTimeLabel&&"CUSTOM"!==t.startTimeLabel?function(e,t){switch(t){case"LAST_HOUR":return{startTime:new Date(new Date(e).setUTCHours((new Date).getUTCHours()-1)).toISOString(),endTime:e.toISOString()};case"LAST_24_HOURS":return{startTime:new Date(new Date(e).setUTCDate((new Date).getUTCDate()-1)).toISOString(),endTime:e.toISOString()};case"LAST_7_DAYS":return{startTime:new Date(new Date(e).setUTCDate((new Date).getUTCDate()-7)).toISOString(),endTime:e.toISOString()};case"LAST_30_DAYS":return{startTime:new Date(new Date(e).setUTCDate((new Date).getUTCDate()-30)).toISOString(),endTime:e.toISOString()};case"LAST_YEAR":return{startTime:new Date(new Date(e).setUTCFullYear((new Date).getUTCFullYear()-1)).toISOString(),endTime:e.toISOString()};case"ALL":return{startTime:void 0,endTime:e.toISOString()};default:throw new Error(`Unexpected start time label: ${t}`)}}(e,t.startTimeLabel):{startTime:t.startTime,endTime:t.endTime}}},23984:function(e,t,n){n.d(t,{I:function(){return h}});var o=n(68248),a=n(27757),i=n(27299),s=n(19112),l=n(91701),r=(n(46709),n(40724)),d=n(73408);var c={name:"ddxhyk",styles:"max-width:800px"},m={name:"ddxhyk",styles:"max-width:800px"};const u={openai:{minVersion:"2.15.1",getContent:()=>(0,d.Y)(r.A,{id:"smtq2M",defaultMessage:"Automatically log traces for OpenAI API calls by calling the {code} function. For example:",values:{code:(0,d.Y)("code",{children:"mlflow.openai.autolog()"})}}),getCodeSource:()=>'from openai import OpenAI\n\nmlflow.openai.autolog()\n\n# Ensure that the "OPENAI_API_KEY" environment variable is set\nclient = OpenAI()\n\nmessages = [\n  {"role": "system", "content": "You are a helpful assistant."},\n  {"role": "user", "content": "Hello!"}\n]\n\n# Inputs and outputs of the API request will be logged in a trace\nclient.chat.completions.create(model="gpt-4o-mini", messages=messages)'},langchain:{minVersion:"2.17.2",getContent:()=>(0,d.Y)(r.A,{id:"7/urtn",defaultMessage:"Automatically log traces for LangChain or LangGraph invocations by calling the {code} function. For example:",values:{code:(0,d.Y)("code",{children:"mlflow.langchain.autolog()"})}}),getCodeSource:()=>'from langchain_openai import OpenAI\nfrom langchain_core.prompts import PromptTemplate\n\nmlflow.langchain.autolog()\n\n# Ensure that the "OPENAI_API_KEY" environment variable is set\nllm = OpenAI()\nprompt = PromptTemplate.from_template("Answer the following question: {question}")\nchain = prompt | llm\n\n# Invoking the chain will cause a trace to be logged\nchain.invoke("What is MLflow?")'},langgraph:{minVersion:"2.19.0",getContent:()=>(0,d.Y)(r.A,{id:"dvL+4V",defaultMessage:"Automatically log traces for LangGraph workflows by calling the {code} function. For example:",values:{code:(0,d.Y)("code",{children:"mlflow.langchain.autolog()"})}}),getCodeSource:()=>'from langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph\nfrom typing import Annotated\n\nmlflow.langchain.autolog()\n\n# Ensure that the "OPENAI_API_KEY" environment variable is set\nmodel = ChatOpenAI(model="gpt-4o-mini")\n\n# Define a minimal LangGraph workflow\nclass GraphState(dict):\n    input: Annotated[str, "input"]\n\ndef call_model(state: GraphState) -> GraphState:\n    response = model.invoke(state["input"])\n    return {"input": state["input"], "response": response.content}\n\ngraph = StateGraph(GraphState)\ngraph.add_node("model", call_model)\ngraph.set_entry_point("model")\napp = graph.compile()\n\n# Executing the graph will log the steps as a trace\napp.invoke({"input": "Say hello to MLflow."})'},llama_index:{minVersion:"2.15.1",getContent:()=>(0,d.Y)(r.A,{id:"/v6KuF",defaultMessage:"Automatically log traces for LlamaIndex queries by calling the {code} function. For example:",values:{code:(0,d.Y)("code",{children:"mlflow.llama_index.autolog()"})}}),getCodeSource:()=>'from llama_index.core import Document, VectorStoreIndex\n\nmlflow.llama_index.autolog()\n\n# Ensure that the "OPENAI_API_KEY" environment variable is set\nindex = VectorStoreIndex.from_documents([Document.example()])\nquery_engine = index.as_query_engine()\n\n# Querying the engine will cause a trace to be logged\nquery_engine.query("What is LlamaIndex?")'},dspy:{minVersion:"2.18.0",getContent:()=>(0,d.Y)(r.A,{id:"h1HQ14",defaultMessage:"Automatically log traces for DSPy executions by calling the {code} function. For example:",values:{code:(0,d.Y)("code",{children:"mlflow.dspy.autolog()"})}}),getCodeSource:()=>'import dspy\n\nmlflow.dspy.autolog()\n\n# Configure the LLM to use. Please ensure that\n# the OPENAI_API_KEY environment variable is set\nlm = dspy.LM("openai/gpt-4o-mini")\ndspy.configure(lm=lm)\n\n# Define a simple chain-of-thought model and run it\nmath = dspy.ChainOfThought("question -> answer: float")\nquestion = "Two dice are tossed. What is the probability that the sum equals two?"\n\n# All intermediate outputs from the execution will be logged\nmath(question=question)'},crewai:{minVersion:"2.19.0",getContent:()=>(0,d.Y)(r.A,{id:"K8LdMX",defaultMessage:"Automatically log traces for CrewAI executions by calling the {code} function. For example:",values:{code:(0,d.Y)("code",{children:"mlflow.crewai.autolog()"})}}),getCodeSource:()=>'from crewai import Agent, Crew, Process, Task\n\nmlflow.crewai.autolog()\n\ncity_selection_agent = Agent(\n    role="City selection expert",\n    goal="Select the best city based on weather, season, and prices",\n    backstory="An expert in analyzing travel data to pick ideal destinations",\n    allow_delegation=True,\n    verbose=True,\n)\n\nlocal_expert = Agent(\n    role="Local expert",\n    goal="Provide the best insights about the selected city",\n    backstory="A local guide with extensive information about the city",\n    verbose=True,\n)\n  \nplan_trip = Task(\n    name="Plan a trip",\n    description="""Plan a trip to a city based on weather, prices, and best local attractions. \n    Please consult with a local expert when researching things to do.""",\n    expected_output="A short summary of the trip destination and key things to do",\n    agent=city_selection_agent,\n)\n\ncrew = Crew(\n  agents=[\n    city_selection_agent,\n    local_expert,\n  ],\n  tasks=[plan_trip],\n  process=Process.sequential\n)\n\n# Ensure the "OPENAI_API_KEY" environment variable is set\n# before kicking off the crew. All intermediate agent outputs\n# will be logged in the resulting trace.\ncrew.kickoff()'},autogen:{minVersion:"2.16.2",getContent:()=>(0,d.Y)(r.A,{id:"i/pJvo",defaultMessage:"Automatically log traces for AutoGen conversations by calling the {code} function. For example:",values:{code:(0,d.Y)("code",{children:"mlflow.autogen.autolog()"})}}),getCodeSource:()=>'import os\nfrom autogen import AssistantAgent, UserProxyAgent\n\nmlflow.autogen.autolog()\n\n# Ensure that the "OPENAI_API_KEY" environment variable is set\nllm_config = { "model": "gpt-4o-mini", "api_key": os.environ["OPENAI_API_KEY"] }\nassistant = AssistantAgent("assistant", llm_config = llm_config)\nuser_proxy = UserProxyAgent("user_proxy", code_execution_config = False)\n\n# All intermediate executions within the chat session will be logged\nuser_proxy.initiate_chat(assistant, message = "What is MLflow?", max_turns = 1)'},anthropic:{minVersion:"2.19.0",getContent:()=>(0,d.Y)(r.A,{id:"AzOnmT",defaultMessage:"Automatically log traces for Anthropic API calls by calling the {code} function. For example:",values:{code:(0,d.Y)("code",{children:"mlflow.anthropic.autolog()"})}}),getCodeSource:()=>'import os\nimport anthropic\n\n# Enable auto-tracing for Anthropic\nmlflow.anthropic.autolog()\n\n# Configure your API key (please ensure that the "ANTHROPIC_API_KEY" environment variable is set)\nclient = anthropic.Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])\n\n# Inputs and outputs of API calls will be logged as a trace\nmessage = client.messages.create(\n    model="claude-3-5-sonnet-20241022",\n    max_tokens=1024,\n    messages=[\n        {"role": "user", "content": "Hello, Claude"},\n    ],\n)'},bedrock:{minVersion:"2.20.0",getContent:()=>(0,d.Y)(r.A,{id:"6tKW1I",defaultMessage:"Automatically log traces for Bedrock conversations by calling the {code} function. For example:",values:{code:(0,d.Y)("code",{children:"mlflow.bedrock.autolog()"})}}),getCodeSource:()=>'import boto3\n\nmlflow.bedrock.autolog()\n\n# Ensure that your boto3 client has the necessary auth information\nbedrock = boto3.client(\n    service_name="bedrock-runtime",\n    region_name="<REPLACE_WITH_YOUR_AWS_REGION>",\n)\n\nmodel = "anthropic.claude-3-5-sonnet-20241022-v2:0"\nmessages = [{ "role": "user", "content": [{"text": "Hello!"}]}]\n\n# All intermediate executions within the chat session will be logged\nbedrock.converse(modelId=model, messages=messages)'},litellm:{minVersion:"2.18.0",getContent:()=>(0,d.Y)(r.A,{id:"D7SSDK",defaultMessage:"Automatically log traces for LiteLLM API calls by calling the {code} function. For example:",values:{code:(0,d.Y)("code",{children:"mlflow.litellm.autolog()"})}}),getCodeSource:()=>'import litellm\n\nmlflow.litellm.autolog()\n\n# Ensure that the "OPENAI_API_KEY" environment variable is set\nmessages = [{"role": "user", "content": "Hello!"}]\n\n# Inputs and outputs of the API request will be logged in a trace\nlitellm.completion(model="gpt-4o-mini", messages=messages)'},gemini:{minVersion:"2.18.0",getContent:()=>(0,d.Y)(r.A,{id:"IsIgE2",defaultMessage:"Automatically log traces for Gemini conversations by calling the {code} function. For example:",values:{code:(0,d.Y)("code",{children:"mlflow.gemini.autolog()"})}}),getCodeSource:()=>'import google.genai as genai\n\nmlflow.gemini.autolog()\n\n# Replace "GEMINI_API_KEY" with your API key\nclient = genai.Client(api_key="GEMINI_API_KEY")\n\n# Inputs and outputs of the API request will be logged in a trace\nclient.models.generate_content(model="gemini-1.5-flash", contents="Hello!")'},custom:{minVersion:"2.14.3",getContent:e=>(0,d.FD)(d.FK,{children:[(0,d.Y)(a.T.Paragraph,{css:c,children:(0,d.Y)(r.A,{id:"3Z6K+n",defaultMessage:"To manually instrument your own traces, the most convenient method is to use the {code} function decorator. This will cause the inputs and outputs of the function to be captured in the trace.",values:{code:(0,d.Y)("code",{children:"@mlflow.trace"})}})}),(0,d.Y)(a.T.Paragraph,{css:m,children:(0,d.Y)(r.A,{id:"WNz02j",defaultMessage:"For more complex use cases, MLflow also provides granular APIs that can be used to control tracing behavior. For more information, please visit the <a>official documentation</a> on fluent and client APIs for MLflow Tracing.",values:{a:t=>(0,d.Y)(a.T.Link,{title:"official documentation",componentId:`${e}.traces_table.custom_tracing_docs_link`,href:"https://mlflow.org/docs/latest/llms/tracing/index.html#tracing-fluent-apis",openInNewTab:!0,children:t})}})})]}),getCodeSource:()=>'@mlflow.trace\ndef foo(a):\n    return a + bar(a)\n\n# Various attributes can be passed to the decorator\n# to modify the information contained in the span\n@mlflow.trace(name = "custom_name", attributes = { "key": "value" })\ndef bar(b):\n    return b + 1\n\n# Invoking the traced function will cause a trace to be logged\nfoo(1)'}};var g={name:"ddxhyk",styles:"max-width:800px"},p={name:"1hyob9y",styles:"position:relative;width:min-content"};const h=({flavorName:e,baseComponentId:t})=>{const{theme:n}=(0,a.u)(),{getContent:r,getCodeSource:c,minVersion:m}=u[e],h=r(t),f=c();return(0,d.FD)("div",{children:[(0,d.Y)(a.T.Text,{css:g,color:"secondary",children:h}),(0,d.FD)("div",{css:p,children:[(0,d.Y)(l.i,{componentId:`${t}.traces_table.${e}_quickstart_snippet_copy`,css:(0,o.AH)({zIndex:1,position:"absolute",top:n.spacing.xs,right:n.spacing.xs},""),showLabel:!1,copyText:f,icon:(0,d.Y)(i.CopyIcon,{})}),(0,d.Y)(s.z7,{showLineNumbers:!0,theme:n.isDarkMode?"duotoneDark":"light",style:{padding:`${n.spacing.sm}px ${n.spacing.md}px`,marginTop:n.spacing.md},language:"python",children:f})]})]})}},39767:function(e,t,n){n.d(t,{i:function(){return m}});var o=n(68248),a=n(27757),i=n(27299),s=n(40724),l=n(76118),r=n(23984),d=n(63394),c=n(73408);const m=({baseComponentId:e,runUuid:t})=>{const{theme:n}=(0,a.u)(),{introductionText:m}=(0,d.S)();return(0,c.FD)("div",{css:(0,o.AH)({overflow:"auto",paddingBottom:n.spacing.lg},""),children:[(0,c.Y)(i.Header,{title:(0,c.Y)(s.A,{id:"6d5JTO",defaultMessage:"No traces recorded"}),titleElementLevel:3}),(0,c.Y)(a.T.Text,{css:(0,o.AH)({display:"block",marginTop:n.spacing.md,marginBottom:n.spacing.md,maxWidth:800},""),children:m||(0,c.Y)(s.A,{id:"msYDmK",defaultMessage:"This tab displays all the traces logged to this {isRun, select, true {run} other {experiment}}. Follow the steps below to log your first trace. For more information about MLflow Tracing, visit the <a>MLflow documentation</a>.",values:{isRun:!(0,l.isNil)(t),a:t=>(0,c.Y)(a.T.Link,{componentId:`${e}.traces_table.quickstart_docs_link`,href:"https://mlflow.org/docs/latest/llms/tracing/index.html",openInNewTab:!0,children:t})}})}),(0,c.Y)(r.I,{flavorName:"custom",baseComponentId:e})]})}},63394:function(e,t,n){n.d(t,{A:function(){return s},S:function(){return l}});var o=n(46709),a=n(73408);const i=(0,o.createContext)({}),s=({children:e,introductionText:t,displayVersionWarnings:n})=>(0,a.Y)(i.Provider,{value:{introductionText:t,displayVersionWarnings:n},children:e}),l=()=>(0,o.useContext)(i)},79444:function(e,t,n){n.d(t,{U:function(){return d},k:function(){return r}});var o=n(76118),a=n(46709),i=n(73408);const s=()=>({dateNow:new Date,lastRefreshTime:Date.now(),refresh:()=>{}}),l=(0,a.createContext)(s()),r=({config:e,children:t})=>{const n=s(),r=(0,o.merge)({},n,e),[d,c]=a.useState(r.lastRefreshTime),m=(0,a.useMemo)((()=>new Date(d)),[d]),u=(0,a.useCallback)((()=>{c(Date.now())}),[]);return(0,i.Y)(l.Provider,{value:{...r,dateNow:m,lastRefreshTime:d,refresh:u},children:t})},d=()=>{const e=(0,a.useContext)(l);return e||s()}},85167:function(e,t,n){n.d(t,{v:function(){return P}});var o=n(46709),a=n(27299),i=n(27757),s=n(28189),l=n(18154),r=n(33656),d=n(31655),c=n(40720),m=n(88525),u=n(42747),g=n(69986),p=n(14429),h=n(66916),f=n(92338),T=n(54741),y=n(91944),A=n(44105),w=n(39767),b=n(73408);const I=e=>{const{experimentIds:t,traceSearchLocations:n,loggedModelId:s,isCallDisabled:r}=e,d=(0,u.tz)(),{data:c,isLoading:m,error:g}=(0,l.Zn)({locations:n,pageSize:1,limit:1,...s?{filterByLoggedModelId:s}:{},disabled:r}),{data:I,loading:v}=(0,T.L)({experimentId:t[0]}),_=I,x=(0,y.BH)(null===_||void 0===_?void 0:_.tags),S=(x===A.Gk.GENAI_DEVELOPMENT||A.Gk.GENAI_DEVELOPMENT_INFERRED,c&&c.length>0),[C,Y]=(0,p.VA)(),M=(0,o.useMemo)((()=>(0,f.p)(d)),[d]),L=(0,b.Y)(i.B,{componentId:"traces-v3-empty-state-button",onClick:()=>Y({startTimeLabel:"ALL"}),children:(0,b.Y)(u.sA,{id:"WpD2MA",defaultMessage:"View All"})});if(m||v)return(0,b.Y)(b.FK,{children:[...Array(10).keys()].map((e=>(0,b.Y)(a.ParagraphSkeleton,{label:"Loading...",seed:`s-${e}`},e)))});if(g)return(0,b.Y)(a.Empty,{image:(0,b.Y)(i.k,{}),title:(0,b.Y)(u.sA,{id:"ZotI2j",defaultMessage:"Fetching traces failed"}),description:String(g)});if(S){var E;const e=(0,b.Y)(h.S,{}),t=(0,b.Y)(u.sA,{id:"XuzIWs",defaultMessage:'Some traces are hidden by your time range filter: "{filterLabel}"',values:{filterLabel:(0,b.Y)("strong",{children:(null===(E=M.find((e=>e.key===C.startTimeLabel)))||void 0===E?void 0:E.label)||""})}});return(0,b.Y)(a.Empty,{title:(0,b.Y)(u.sA,{id:"Dhr3pC",defaultMessage:"No traces found"}),description:t,button:L,image:e})}return(0,b.Y)(w.i,{baseComponentId:"mlflow.traces"})};var v=n(39595),_=n(91105);var x=n(82636),S=n(52855),C=n(27462);const Y=({children:e,makeHtmlFromMarkdown:t,experimentId:n})=>(0,b.Y)(l.tU,{makeHtml:t,children:e});var M={name:"1dd35ky",styles:"display:flex;flex:1;overflow:hidden"},L={name:"1dd35ky",styles:"display:flex;flex:1;overflow:hidden"},E={name:"db3fum",styles:"display:flex;flex-direction:column;width:100%;gap:8px;padding:16px"},k={name:"1w1qal4",styles:"display:flex;align-items:center;justify-content:center;width:100%;height:100%"},D={name:"1nxh63r",styles:"overflow-y:hidden;height:100%;display:flex;flex-direction:column"};const P=o.memo((({experimentId:e,endpointName:t,timeRange:n,isLoadingExperiment:h,loggedModelId:f})=>{const T=(0,r.N9)(),y=(0,u.tz)(),A=(0,d.rE)(),w=(0,o.useMemo)((()=>[(0,l.$U)(e)]),[e]),P=!1,O=g.Uv,{assessmentInfos:F,allColumns:N,totalCount:R,evaluatedTraces:U,isLoading:H,error:V,isEmpty:q,tableFilterOptions:K}=(0,l.KW)({locations:w,timeRange:n,filterByLoggedModelId:f,disabled:P}),[G,$]=(0,o.useState)(""),[W,B]=(0,l.R7)(),z=(0,v.jE)(),Q=(0,o.useCallback)((e=>{const{responseHasContent:t,inputHasContent:n,tokensHasContent:o}=(0,x.l)(U);return e.filter((e=>e.type===l.$6.ASSESSMENT||e.type===l.$6.EXPECTATION||n&&e.type===l.$6.INPUT||t&&e.type===l.$6.TRACE_INFO&&e.id===l.Rl||o&&e.type===l.$6.TRACE_INFO&&e.id===l.YO||e.type===l.$6.TRACE_INFO&&[l.XQ,l.tj,l.Te,l.$W].includes(e.id)||e.type===l.$6.INTERNAL_MONITOR_REQUEST_TIME))}),[U]),{selectedColumns:j,toggleColumns:X,setSelectedColumns:Z}=(0,l.K0)(e,N,Q),[J,ee]=(0,l.GY)(j,{key:l.Te,type:l.$6.TRACE_INFO,asc:!1}),{isInitialTimeFilterLoading:te}=(({locations:e,isTracesEmpty:t,isTraceMetadataLoading:n,sqlWarehouseId:o,disabled:a=!1})=>{const[i]=(0,_.ok)(),[s,r]=(0,p.VA)(),d=t&&!n&&!i.has(p.vF),{data:c,isLoading:m}=(0,l.Zn)({locations:e,tableSort:{key:l.Te,type:l.$6.TRACE_INFO,asc:!1},disabled:!d||a,limit:500,pageSize:500,sqlWarehouseId:o});d&&c&&c.length>0&&!m&&r({startTimeLabel:"CUSTOM",startTime:c[c.length-1].request_time,endTime:(new Date).toISOString()});return{isInitialTimeFilterLoading:d&&m}})({locations:w,isTracesEmpty:q,isTraceMetadataLoading:H,disabled:P}),{data:ne,isLoading:oe,isFetching:ae,error:ie}=(0,l.Zn)({locations:w,currentRunDisplayName:t,searchQuery:G,filters:W,timeRange:n,filterByLoggedModelId:f,tableSort:J,disabled:P}),{showEditTagsModalForTrace:se,EditTagsModal:le}=((0,c.C)(),(0,m.$)({onSuccess:()=>(0,l.BL)({queryClient:z}),existingTagKeys:(0,l.d9)(ne||[])})),{showTagAssignmentModal:re,TagAssignmentModal:de}=(0,s.BR)({componentIdPrefix:"mlflow.experiment-traces",onSuccess:()=>(0,l.BL)({queryClient:z})}),ce=(0,C.F)({traceSearchLocations:w}),{showExportTracesToDatasetsModal:me,setShowExportTracesToDatasetsModal:ue,renderExportTracesToDatasetsModal:ge}=(0,S.c)({experimentId:e}),pe=(0,o.useMemo)((()=>({deleteTracesAction:ce,exportToEvals:{showExportTracesToDatasetsModal:me,setShowExportTracesToDatasetsModal:ue,renderExportTracesToDatasetsModal:ge},editTags:(0,s.F9)()?{showEditTagsModalForTrace:re,EditTagsModal:de}:{showEditTagsModalForTrace:se,EditTagsModal:le}})),[ce,me,ue,ge,re,de,se,le]),he=(0,o.useMemo)((()=>({currentCount:null===ne||void 0===ne?void 0:ne.length,logCountLoading:oe,totalCount:R,maxAllowedCount:(0,l.pR)()})),[ne,R,oe]),fe=oe||te||H,Te=ie||V,ye=q&&!fe&&!Te;return(0,b.Y)(l.sG,{children:(0,b.FD)("div",{css:D,children:[(0,b.Y)(l.w_,{experimentId:e,searchQuery:G,setSearchQuery:$,filters:W,setFilters:B,assessmentInfos:F,traceInfos:ne,tableFilterOptions:K,countInfo:he,traceActions:pe,tableSort:J,setTableSort:ee,allColumns:N,selectedColumns:j,toggleColumns:X,setSelectedColumns:Z,isMetadataLoading:H,metadataError:V,usesV4APIs:!0}),!A&&ye?(0,b.Y)(I,{experimentIds:[e],loggedModelId:f,traceSearchLocations:w,isCallDisabled:P}):(0,b.Y)("div",{css:M,children:(0,b.Y)("div",{css:L,children:fe?(0,b.Y)("div",{css:E,children:[...Array(10).keys()].map((e=>(0,b.Y)(a.ParagraphSkeleton,{label:"Loading...",seed:`s-${e}`},e)))}):Te?(0,b.Y)("div",{css:k,children:(0,b.Y)(a.Empty,{image:(0,b.Y)(i.k,{}),title:y.formatMessage({id:"Bcr4hl",defaultMessage:"Fetching traces failed"}),description:Te.message})}):(0,b.Y)(Y,{makeHtmlFromMarkdown:T,experimentId:e,children:(0,b.Y)(l._p,{experimentId:e,allColumns:N,currentTraceInfoV3:ne||[],currentRunDisplayName:t,getTrace:O,assessmentInfos:F,setFilters:B,filters:W,selectedColumns:j,tableSort:J,onTraceTagsEdit:se,displayLoadingOverlay:!1})})})})]})})}))},92338:function(e,t,n){function o(e){return[{key:"LAST_HOUR",label:e.formatMessage({id:"bjoGjg",defaultMessage:"Last hour"})},{key:"LAST_24_HOURS",label:e.formatMessage({id:"JChRnq",defaultMessage:"Last 24 hours"})},{key:"LAST_7_DAYS",label:e.formatMessage({id:"eQsXm7",defaultMessage:"Last 7 days"})},{key:"LAST_30_DAYS",label:e.formatMessage({id:"p1H1KR",defaultMessage:"Last 30 days"})},{key:"LAST_YEAR",label:e.formatMessage({id:"EI7moF",defaultMessage:"Last year"})},{key:"ALL",label:e.formatMessage({id:"HeNa8H",defaultMessage:"All"})},{key:"CUSTOM",label:e.formatMessage({id:"TpmJlu",defaultMessage:"Custom"})}]}n.d(t,{p:function(){return o}})}}]);